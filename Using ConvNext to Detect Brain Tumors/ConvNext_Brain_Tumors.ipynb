{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXxV9dgFyjbN"
      },
      "source": [
        "# Installing and import of libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UhERoe4jyuZ1",
        "outputId": "634e071c-a9e9-4278-a7b7-ba3681586e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting icrawler\n",
            "  Downloading icrawler-0.6.10-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from icrawler) (4.13.4)\n",
            "Collecting bs4 (from icrawler)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from icrawler) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from icrawler) (11.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from icrawler) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from icrawler) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from icrawler) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->icrawler) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->icrawler) (2025.4.26)\n",
            "Downloading icrawler-0.6.10-py3-none-any.whl (36 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4, icrawler\n",
            "Successfully installed bs4-0.0.2 icrawler-0.6.10\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install icrawler # for web scraping\n",
        "!pip install pillow # for image manipulation\n",
        "!pip install opencv-python # for image manipulation\n",
        "!pip install timm scikit-learn # for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NYd4Vzbyq4l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "from google.colab import drive\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEkznc-D0mId"
      },
      "source": [
        "Checking CUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU6Fhauj0lm8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "print(\"Is CUDA available?:\", torch.cuda.is_available())\n",
        "\n",
        "# Number of available GPUs\n",
        "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "\n",
        "# Device name\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Current device:\", torch.cuda.current_device())\n",
        "else:\n",
        "    print(\"No GPU - running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F3Da2UZ0vyt"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Drive mount\n",
        "```\n",
        "\n",
        "Zamontowanie dysku"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ErJkr5B0x-R"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1N6vd5fyQ7P"
      },
      "source": [
        "# Data download and processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytl4FUZXyWvU"
      },
      "source": [
        "Creating folders in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzeyF8SnhVti"
      },
      "outputs": [],
      "source": [
        "# Base path\n",
        "base_path = \"/content/drive/MyDrive/DL\"\n",
        "\n",
        "# List of folder names\n",
        "folders = [\n",
        "    \"BT_glioma\",\n",
        "    \"BT_meningioma\",\n",
        "    \"BT_pituitary\",\n",
        "    \"MS\",\n",
        "    \"Normal\",\n",
        "]\n",
        "\n",
        "# Creating folders\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "print(\"Folders have been created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq_4Fn-Bxes_",
        "outputId": "2aa6cb86-1e2a-448a-e094-e01a73ef5241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skopiowano 300 plików z BT_glioma\n",
            "Skopiowano 300 plików z BT_meningioma\n",
            "Skopiowano 300 plików z BT_pituitary\n",
            "Skopiowano 300 plików z MS\n",
            "Skopiowano 300 plików z Normal\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Source and destination paths\n",
        "src_base = '/content/drive/MyDrive/DL/MRI_scans'\n",
        "dst_base = '/content/drive/MyDrive/DL/Brain_scans'\n",
        "\n",
        "# List of folders/categories\n",
        "categories = ['BT_glioma', 'BT_meningioma', 'BT_pituitary', 'MS', 'Normal']\n",
        "\n",
        "# Creating destination folder and subfolders\n",
        "os.makedirs(dst_base, exist_ok=True)\n",
        "\n",
        "for category in categories:\n",
        "    src_folder = os.path.join(src_base, category)\n",
        "    dst_folder = os.path.join(dst_base, category)\n",
        "    os.makedirs(dst_folder, exist_ok=True)\n",
        "\n",
        "    # Get list of files (only files, not folders)\n",
        "    files = [f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f))]\n",
        "\n",
        "    # Copy up to 300 files\n",
        "    for file_name in files[:300]:\n",
        "        src_file = os.path.join(src_folder, file_name)\n",
        "        dst_file = os.path.join(dst_folder, file_name)\n",
        "        shutil.copy2(src_file, dst_file)\n",
        "\n",
        "    print(f'Copied {min(300, len(files))} files from {category}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CaAvKMn1dTQ"
      },
      "source": [
        "For each folder containing images of a specific brain tumor, automatic dataset augmentation is performed.\n",
        "The goal is to reach a target number of images (e.g., 200) by creating synthetic examples from existing ones.\n",
        "To achieve this, random (more precisely: pseudorandom) transformations are applied to selected images.\n",
        "\n",
        "Transformations include:\n",
        "\n",
        "- Mirror flip – horizontal flip of the image (ImageOps.mirror),\n",
        "\n",
        "- Rotation – rotation by a random angle chosen from: 90°, 180°, or 270°,\n",
        "\n",
        "- Cropping with rescaling – removing 10% of the edges from each side and rescaling to the original size,\n",
        "\n",
        "- Color quantization – reducing the number of colors (randomly between 4–16),\n",
        "\n",
        "- Gaussian blur – simulating blur with a filter of random radius (1–3 px),\n",
        "\n",
        "- Gaussian noise – adding random noise to the image (simulating distortions),\n",
        "\n",
        "- Shear – distorting the image by horizontal shearing,\n",
        "\n",
        "- Random crop with rescaling – cropping a random region covering 80% of the image and rescaling it back to the original size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H8MaZaRp3Yn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import uuid\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = \"/content/drive/MyDrive/DL/Brain_scans/Normal\"\n",
        "\n",
        "# Target number\n",
        "target_count = 350\n",
        "\n",
        "# Possible transformations\n",
        "def apply_random_transformation(img):\n",
        "    transformation = random.choice([\n",
        "        'mirror', 'rotate', 'crop', 'quantize',\n",
        "        'blur', 'noise', 'shear', 'random_crop'\n",
        "    ])\n",
        "\n",
        "    if transformation == 'mirror':\n",
        "        return ImageOps.mirror(img)\n",
        "\n",
        "    elif transformation == 'rotate':\n",
        "        angle = random.choice([90, 180, 270])\n",
        "        return img.rotate(angle)\n",
        "\n",
        "    elif transformation == 'crop':\n",
        "        width, height = img.size\n",
        "        left = width * 0.1\n",
        "        top = height * 0.1\n",
        "        right = width * 0.9\n",
        "        bottom = height * 0.9\n",
        "        return img.crop((left, top, right, bottom)).resize((width, height))\n",
        "\n",
        "    elif transformation == 'quantize':\n",
        "        return img.quantize(colors=random.randint(4, 16)).convert('RGB')\n",
        "\n",
        "    elif transformation == 'blur':\n",
        "        return img.filter(ImageFilter.GaussianBlur(radius=random.uniform(1, 3)))\n",
        "\n",
        "    elif transformation == 'noise':\n",
        "        img_np = np.array(img)\n",
        "        noise = np.random.normal(0, 25, img_np.shape).astype(np.uint8)\n",
        "        noisy = cv2.add(img_np, noise)\n",
        "        return Image.fromarray(noisy)\n",
        "\n",
        "    elif transformation == 'shear':\n",
        "        width, height = img.size\n",
        "        m = 0.2  # shearing factor\n",
        "        xshift = abs(m) * width\n",
        "        new_width = width + int(round(xshift))\n",
        "        return img.transform((new_width, height), Image.AFFINE,\n",
        "                             (1, m, -xshift if m > 0 else 0, 0, 1, 0),\n",
        "                             Image.BICUBIC)\n",
        "\n",
        "    elif transformation == 'random_crop':\n",
        "        width, height = img.size\n",
        "        crop_size = int(min(width, height) * 0.8)\n",
        "        left = random.randint(0, width - crop_size)\n",
        "        top = random.randint(0, height - crop_size)\n",
        "        right = left + crop_size\n",
        "        bottom = top + crop_size\n",
        "        return img.crop((left, top, right, bottom)).resize((width, height))\n",
        "\n",
        "    return img\n",
        "\n",
        "# Main loop\n",
        "existing_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png'))]\n",
        "image_count = len(existing_files)\n",
        "\n",
        "# List of original images for augmentation\n",
        "base_images = [os.path.join(folder_path, f) for f in existing_files]\n",
        "\n",
        "if not base_images:\n",
        "    print(\"No original images available for transformation.\")\n",
        "else:\n",
        "    while image_count < target_count:\n",
        "        img_path = random.choice(base_images)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        transformed = apply_random_transformation(img)\n",
        "        new_filename = f\"{uuid.uuid4().hex}.jpg\"\n",
        "        transformed.save(os.path.join(folder_path, new_filename))\n",
        "\n",
        "        image_count += 1\n",
        "\n",
        "    print(f\"Completed – the folder now contains {image_count} images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et2ADJmi2i3h"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-JXP8gG2kzv"
      },
      "source": [
        "Helper function to save files in CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbnNBMQUUqRy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Function to initialize the CSV file (with header)\n",
        "def init_csv(output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    csv_file = os.path.join(output_dir, \"training_metrics.csv\")\n",
        "    if not os.path.exists(csv_file):\n",
        "        with open(csv_file, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\n",
        "                \"Epoch\", \"Epoch Time (s)\",\n",
        "                \"Train Accuracy\", \"Train Loss\", \"Train F1\",\n",
        "                \"Validation Accuracy\", \"Validation Loss\", \"Validation F1\"\n",
        "            ])\n",
        "    return csv_file  # <- return the path to the file\n",
        "\n",
        "# Function to log metrics after each epoch\n",
        "def log_metrics(output_dir, epoch, epoch_time,\n",
        "                train_acc, train_loss, train_f1,\n",
        "                val_acc, val_loss, val_f1):\n",
        "\n",
        "    csv_file = os.path.join(output_dir, \"training_metrics.csv\")\n",
        "\n",
        "    with open(csv_file, mode='a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\n",
        "            epoch, f\"{epoch_time:.2f}\",\n",
        "            f\"{train_acc:.4f}\", f\"{train_loss:.4f}\", f\"{train_f1:.4f}\",\n",
        "            f\"{val_acc:.4f}\", f\"{val_loss:.4f}\", f\"{val_f1:.4f}\"\n",
        "        ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tVUr6082qeO"
      },
      "source": [
        "Helper function to plot charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCmcaBDHVlwD"
      },
      "outputs": [],
      "source": [
        "def save_epoch_plots(epoch, train_acc, val_acc, train_loss, val_loss, model, val_loader, class_names, device, output_dir):\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    epochs_range = range(1, len(history['train_acc']) + 1\n",
        ")\n",
        "\n",
        "    # Wykres Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_range, history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy per Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, f\"accuracy_epoch_{epoch+1}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Wykres Loss\n",
        "    plt.figure()\n",
        "    plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
        "    plt.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss per Epoch')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, f\"loss_epoch_{epoch+1}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # Macierz pomyłek\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix (Epoch {epoch+1})')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, f\"confusion_matrix_epoch_{epoch+1}.png\"))\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7WsdyDX2v54"
      },
      "source": [
        "Training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5nXd43vs7d-",
        "outputId": "b7ad745e-d751-418f-b599-1af5fa92eed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Używane urządzenie: cuda\n",
            "\n",
            "========================> Trenowanie: lr=0.001, drop_rate=0.3, batch_size=16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 77/77 [00:07<00:00, 10.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.1271, acc: 0.5527, f1: 0.5484\n",
            "Val   loss: 0.7290, acc: 0.8015, f1: 0.7994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 77/77 [00:07<00:00, 10.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 0.7853, acc: 0.7061, f1: 0.7027\n",
            "Val   loss: 0.5957, acc: 0.8206, f1: 0.8258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 77/77 [00:07<00:00, 10.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 0.6819, acc: 0.7567, f1: 0.7566\n",
            "Val   loss: 0.5038, acc: 0.8511, f1: 0.8492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 77/77 [00:05<00:00, 13.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 0.6084, acc: 0.7755, f1: 0.7740\n",
            "Val   loss: 0.4727, acc: 0.8473, f1: 0.8499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 77/77 [00:05<00:00, 12.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 0.5876, acc: 0.7927, f1: 0.7921\n",
            "Val   loss: 0.4409, acc: 0.8664, f1: 0.8675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 77/77 [00:07<00:00, 10.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 0.5489, acc: 0.7861, f1: 0.7855\n",
            "Val   loss: 0.4220, acc: 0.8473, f1: 0.8531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 77/77 [00:07<00:00, 10.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 0.5438, acc: 0.8049, f1: 0.8036\n",
            "Val   loss: 0.4244, acc: 0.8550, f1: 0.8612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 77/77 [00:05<00:00, 13.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 0.5341, acc: 0.8008, f1: 0.8002\n",
            "Val   loss: 0.3936, acc: 0.8817, f1: 0.8814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 77/77 [00:05<00:00, 13.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 0.5321, acc: 0.7902, f1: 0.7901\n",
            "Val   loss: 0.3803, acc: 0.8702, f1: 0.8682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 77/77 [00:07<00:00, 10.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 0.5240, acc: 0.8008, f1: 0.8005\n",
            "Val   loss: 0.4112, acc: 0.8664, f1: 0.8693\n",
            "\n",
            "== TEST SET dla lr=0.001, drop_rate=0.3, batch_size=16 ==\n",
            "Test loss: 0.4618, acc: 0.8555, f1: 0.8590\n",
            "\n",
            "========================> Trenowanie: lr=0.001, drop_rate=0.3, batch_size=32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 39/39 [00:06<00:00,  5.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.2407, acc: 0.5037, f1: 0.5046\n",
            "Val   loss: 0.9372, acc: 0.6908, f1: 0.6908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 39/39 [00:05<00:00,  6.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 0.8469, acc: 0.6922, f1: 0.6915\n",
            "Val   loss: 0.7627, acc: 0.7366, f1: 0.7256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 39/39 [00:05<00:00,  7.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 0.7238, acc: 0.7453, f1: 0.7422\n",
            "Val   loss: 0.7146, acc: 0.7328, f1: 0.7357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 39/39 [00:07<00:00,  5.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 0.6886, acc: 0.7584, f1: 0.7581\n",
            "Val   loss: 0.6820, acc: 0.7481, f1: 0.7516\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 39/39 [00:06<00:00,  5.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 0.6371, acc: 0.7755, f1: 0.7745\n",
            "Val   loss: 0.6375, acc: 0.7710, f1: 0.7702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 39/39 [00:05<00:00,  6.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 0.5904, acc: 0.7910, f1: 0.7908\n",
            "Val   loss: 0.6118, acc: 0.7786, f1: 0.7772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 39/39 [00:05<00:00,  6.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 0.5736, acc: 0.7894, f1: 0.7895\n",
            "Val   loss: 0.6055, acc: 0.7977, f1: 0.7933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 39/39 [00:06<00:00,  5.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 0.5693, acc: 0.7878, f1: 0.7865\n",
            "Val   loss: 0.5714, acc: 0.7863, f1: 0.7878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 39/39 [00:06<00:00,  5.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 0.5333, acc: 0.8114, f1: 0.8099\n",
            "Val   loss: 0.5692, acc: 0.8168, f1: 0.8208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 39/39 [00:05<00:00,  6.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 0.5298, acc: 0.7951, f1: 0.7954\n",
            "Val   loss: 0.5583, acc: 0.8015, f1: 0.8047\n",
            "\n",
            "== TEST SET dla lr=0.001, drop_rate=0.3, batch_size=32 ==\n",
            "Test loss: 0.4131, acc: 0.8669, f1: 0.8651\n",
            "\n",
            "========================> Trenowanie: lr=0.001, drop_rate=0.5, batch_size=16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 77/77 [00:05<00:00, 13.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.2006, acc: 0.5086, f1: 0.5085\n",
            "Val   loss: 0.8218, acc: 0.7061, f1: 0.7033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 77/77 [00:07<00:00, 10.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 0.8609, acc: 0.6686, f1: 0.6651\n",
            "Val   loss: 0.6736, acc: 0.7595, f1: 0.7502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 77/77 [00:07<00:00, 10.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 0.7485, acc: 0.7233, f1: 0.7210\n",
            "Val   loss: 0.6185, acc: 0.7710, f1: 0.7575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 77/77 [00:05<00:00, 13.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 0.7112, acc: 0.7314, f1: 0.7274\n",
            "Val   loss: 0.5719, acc: 0.7863, f1: 0.7722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 77/77 [00:05<00:00, 13.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 0.6669, acc: 0.7518, f1: 0.7510\n",
            "Val   loss: 0.5172, acc: 0.8282, f1: 0.8230\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 77/77 [00:07<00:00, 10.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 0.6287, acc: 0.7567, f1: 0.7534\n",
            "Val   loss: 0.5144, acc: 0.8092, f1: 0.8031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 77/77 [00:06<00:00, 11.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 0.6336, acc: 0.7535, f1: 0.7527\n",
            "Val   loss: 0.4821, acc: 0.8359, f1: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 77/77 [00:05<00:00, 13.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 0.6374, acc: 0.7420, f1: 0.7402\n",
            "Val   loss: 0.4848, acc: 0.8130, f1: 0.8026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 77/77 [00:05<00:00, 13.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 0.6275, acc: 0.7396, f1: 0.7363\n",
            "Val   loss: 0.4699, acc: 0.8244, f1: 0.8185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 77/77 [00:07<00:00, 10.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 0.6675, acc: 0.7518, f1: 0.7501\n",
            "Val   loss: 0.4579, acc: 0.8206, f1: 0.8163\n",
            "\n",
            "== TEST SET dla lr=0.001, drop_rate=0.5, batch_size=16 ==\n",
            "Test loss: 0.4778, acc: 0.8289, f1: 0.8274\n",
            "\n",
            "========================> Trenowanie: lr=0.001, drop_rate=0.5, batch_size=32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 39/39 [00:06<00:00,  5.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.3006, acc: 0.4490, f1: 0.4481\n",
            "Val   loss: 0.8719, acc: 0.7099, f1: 0.6508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 39/39 [00:05<00:00,  6.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 0.9212, acc: 0.6669, f1: 0.6607\n",
            "Val   loss: 0.7016, acc: 0.7634, f1: 0.7440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 39/39 [00:05<00:00,  6.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 0.8171, acc: 0.7029, f1: 0.7028\n",
            "Val   loss: 0.6015, acc: 0.7977, f1: 0.7892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 39/39 [00:06<00:00,  5.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 0.7740, acc: 0.7053, f1: 0.7034\n",
            "Val   loss: 0.5714, acc: 0.7977, f1: 0.7884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 39/39 [00:06<00:00,  5.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 0.7220, acc: 0.7355, f1: 0.7315\n",
            "Val   loss: 0.5335, acc: 0.8130, f1: 0.8065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 39/39 [00:05<00:00,  6.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 0.7017, acc: 0.7249, f1: 0.7235\n",
            "Val   loss: 0.5025, acc: 0.8168, f1: 0.8150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 39/39 [00:05<00:00,  6.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 0.6820, acc: 0.7510, f1: 0.7494\n",
            "Val   loss: 0.4985, acc: 0.8092, f1: 0.8013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 39/39 [00:06<00:00,  5.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 0.6553, acc: 0.7518, f1: 0.7506\n",
            "Val   loss: 0.4824, acc: 0.8206, f1: 0.8115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 39/39 [00:06<00:00,  5.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 0.6624, acc: 0.7486, f1: 0.7467\n",
            "Val   loss: 0.4772, acc: 0.8206, f1: 0.8110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 39/39 [00:05<00:00,  6.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 0.6231, acc: 0.7584, f1: 0.7571\n",
            "Val   loss: 0.4491, acc: 0.8473, f1: 0.8453\n",
            "\n",
            "== TEST SET dla lr=0.001, drop_rate=0.5, batch_size=32 ==\n",
            "Test loss: 0.4403, acc: 0.8479, f1: 0.8465\n",
            "\n",
            "========================> Trenowanie: lr=0.0001, drop_rate=0.3, batch_size=16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 77/77 [00:05<00:00, 13.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.5757, acc: 0.2727, f1: 0.2659\n",
            "Val   loss: 1.4474, acc: 0.4809, f1: 0.4504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 77/77 [00:06<00:00, 11.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 1.3826, acc: 0.4580, f1: 0.4529\n",
            "Val   loss: 1.2809, acc: 0.5725, f1: 0.5358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 77/77 [00:07<00:00, 10.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 1.2476, acc: 0.5567, f1: 0.5465\n",
            "Val   loss: 1.1516, acc: 0.6527, f1: 0.6335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 77/77 [00:06<00:00, 12.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 1.1603, acc: 0.6057, f1: 0.6003\n",
            "Val   loss: 1.0623, acc: 0.6985, f1: 0.6873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 77/77 [00:05<00:00, 13.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 1.0616, acc: 0.6653, f1: 0.6572\n",
            "Val   loss: 0.9950, acc: 0.7061, f1: 0.6935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 77/77 [00:06<00:00, 11.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 1.0112, acc: 0.6522, f1: 0.6464\n",
            "Val   loss: 0.9400, acc: 0.7252, f1: 0.7102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 77/77 [00:07<00:00, 10.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 0.9796, acc: 0.6653, f1: 0.6555\n",
            "Val   loss: 0.8981, acc: 0.7328, f1: 0.7172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 77/77 [00:06<00:00, 12.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 0.9273, acc: 0.6955, f1: 0.6912\n",
            "Val   loss: 0.8577, acc: 0.7481, f1: 0.7368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 77/77 [00:05<00:00, 13.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 0.8819, acc: 0.7265, f1: 0.7201\n",
            "Val   loss: 0.8265, acc: 0.7443, f1: 0.7333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 77/77 [00:06<00:00, 11.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 0.8649, acc: 0.7078, f1: 0.7036\n",
            "Val   loss: 0.8027, acc: 0.7557, f1: 0.7432\n",
            "\n",
            "== TEST SET dla lr=0.0001, drop_rate=0.3, batch_size=16 ==\n",
            "Test loss: 0.7556, acc: 0.7833, f1: 0.7787\n",
            "\n",
            "========================> Trenowanie: lr=0.0001, drop_rate=0.3, batch_size=32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 39/39 [00:06<00:00,  5.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.6015, acc: 0.2808, f1: 0.2782\n",
            "Val   loss: 1.4818, acc: 0.3282, f1: 0.3462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 39/39 [00:06<00:00,  6.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 1.4611, acc: 0.3820, f1: 0.3862\n",
            "Val   loss: 1.3560, acc: 0.4771, f1: 0.5023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 39/39 [00:05<00:00,  6.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 1.3725, acc: 0.4302, f1: 0.4374\n",
            "Val   loss: 1.2550, acc: 0.5687, f1: 0.5796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 39/39 [00:06<00:00,  6.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 1.2941, acc: 0.4776, f1: 0.4832\n",
            "Val   loss: 1.1745, acc: 0.5992, f1: 0.6115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 39/39 [00:06<00:00,  5.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 1.2155, acc: 0.5412, f1: 0.5436\n",
            "Val   loss: 1.1073, acc: 0.6489, f1: 0.6550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 39/39 [00:05<00:00,  6.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 1.1588, acc: 0.5812, f1: 0.5824\n",
            "Val   loss: 1.0480, acc: 0.6756, f1: 0.6798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 39/39 [00:05<00:00,  6.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 1.1126, acc: 0.6188, f1: 0.6210\n",
            "Val   loss: 0.9978, acc: 0.6832, f1: 0.6864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 39/39 [00:06<00:00,  5.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 1.0756, acc: 0.6269, f1: 0.6275\n",
            "Val   loss: 0.9538, acc: 0.6947, f1: 0.6968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 39/39 [00:06<00:00,  5.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 1.0330, acc: 0.6351, f1: 0.6362\n",
            "Val   loss: 0.9179, acc: 0.7061, f1: 0.7055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 39/39 [00:05<00:00,  6.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 1.0060, acc: 0.6539, f1: 0.6537\n",
            "Val   loss: 0.8858, acc: 0.7099, f1: 0.7092\n",
            "\n",
            "== TEST SET dla lr=0.0001, drop_rate=0.3, batch_size=32 ==\n",
            "Test loss: 0.9030, acc: 0.7567, f1: 0.7554\n",
            "\n",
            "========================> Trenowanie: lr=0.0001, drop_rate=0.5, batch_size=16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 77/77 [00:05<00:00, 13.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.5808, acc: 0.2865, f1: 0.2902\n",
            "Val   loss: 1.4222, acc: 0.4733, f1: 0.4755\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 77/77 [00:05<00:00, 12.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 1.4139, acc: 0.4057, f1: 0.4059\n",
            "Val   loss: 1.2711, acc: 0.5954, f1: 0.5957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 77/77 [00:06<00:00, 11.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 1.3018, acc: 0.4857, f1: 0.4839\n",
            "Val   loss: 1.1551, acc: 0.6641, f1: 0.6695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 77/77 [00:07<00:00, 10.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 1.2067, acc: 0.5314, f1: 0.5327\n",
            "Val   loss: 1.0737, acc: 0.6756, f1: 0.6851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 77/77 [00:07<00:00, 10.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 1.1533, acc: 0.5665, f1: 0.5671\n",
            "Val   loss: 1.0093, acc: 0.6985, f1: 0.7042\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 77/77 [00:06<00:00, 12.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 1.0727, acc: 0.6049, f1: 0.6069\n",
            "Val   loss: 0.9465, acc: 0.7443, f1: 0.7486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 77/77 [00:05<00:00, 13.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 1.0327, acc: 0.6327, f1: 0.6330\n",
            "Val   loss: 0.8998, acc: 0.7481, f1: 0.7508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 77/77 [00:05<00:00, 13.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 0.9919, acc: 0.6367, f1: 0.6362\n",
            "Val   loss: 0.8649, acc: 0.7519, f1: 0.7551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 77/77 [00:07<00:00, 10.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 0.9682, acc: 0.6351, f1: 0.6359\n",
            "Val   loss: 0.8285, acc: 0.7595, f1: 0.7601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 77/77 [00:07<00:00, 10.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 0.9234, acc: 0.6661, f1: 0.6664\n",
            "Val   loss: 0.8017, acc: 0.7786, f1: 0.7810\n",
            "\n",
            "== TEST SET dla lr=0.0001, drop_rate=0.5, batch_size=16 ==\n",
            "Test loss: 0.8329, acc: 0.7529, f1: 0.7521\n",
            "\n",
            "========================> Trenowanie: lr=0.0001, drop_rate=0.5, batch_size=32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: 100%|██████████| 39/39 [00:06<00:00,  5.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 1.7318, acc: 0.2057, f1: 0.1964\n",
            "Val   loss: 1.5246, acc: 0.3893, f1: 0.3567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|██████████| 39/39 [00:05<00:00,  7.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2\n",
            "Train loss: 1.5854, acc: 0.2759, f1: 0.2733\n",
            "Val   loss: 1.3874, acc: 0.4924, f1: 0.4613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|██████████| 39/39 [00:05<00:00,  7.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 3\n",
            "Train loss: 1.4609, acc: 0.3755, f1: 0.3717\n",
            "Val   loss: 1.2784, acc: 0.5649, f1: 0.5311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|██████████| 39/39 [00:06<00:00,  6.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 4\n",
            "Train loss: 1.3898, acc: 0.4294, f1: 0.4250\n",
            "Val   loss: 1.1901, acc: 0.6336, f1: 0.6065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10: 100%|██████████| 39/39 [00:07<00:00,  5.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 5\n",
            "Train loss: 1.2818, acc: 0.5086, f1: 0.5049\n",
            "Val   loss: 1.1209, acc: 0.6489, f1: 0.6236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10: 100%|██████████| 39/39 [00:06<00:00,  6.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 6\n",
            "Train loss: 1.2602, acc: 0.5143, f1: 0.5091\n",
            "Val   loss: 1.0628, acc: 0.6527, f1: 0.6268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10: 100%|██████████| 39/39 [00:05<00:00,  6.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 7\n",
            "Train loss: 1.1914, acc: 0.5510, f1: 0.5458\n",
            "Val   loss: 1.0137, acc: 0.6756, f1: 0.6556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10: 100%|██████████| 39/39 [00:05<00:00,  6.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 8\n",
            "Train loss: 1.1375, acc: 0.5600, f1: 0.5552\n",
            "Val   loss: 0.9705, acc: 0.6985, f1: 0.6856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10: 100%|██████████| 39/39 [00:06<00:00,  6.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 9\n",
            "Train loss: 1.1220, acc: 0.5682, f1: 0.5649\n",
            "Val   loss: 0.9344, acc: 0.7214, f1: 0.7124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10: 100%|██████████| 39/39 [00:06<00:00,  5.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 10\n",
            "Train loss: 1.0730, acc: 0.5902, f1: 0.5882\n",
            "Val   loss: 0.9026, acc: 0.7252, f1: 0.7179\n",
            "\n",
            "== TEST SET dla lr=0.0001, drop_rate=0.5, batch_size=32 ==\n",
            "Test loss: 0.9734, acc: 0.7186, f1: 0.7159\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ======================== DATA PATH\n",
        "data_path = \"/content/drive/MyDrive/DL/Brain_scans/\"\n",
        "\n",
        "# ======================== TRANSFORMATIONS\n",
        "\"\"\"\n",
        "Selecting the computation device (GPU if available, otherwise CPU),\n",
        "setting the path to the folder with MRI data, and preparing image transformations.\n",
        "Images are resized to 224x224 pixels, converted to tensors, and normalized to the range [-1, 1].\n",
        "\"\"\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "\n",
        "# ======================== LOADING DATA\n",
        "\"\"\"\n",
        "Loading the entire image dataset from subfolders, where each folder represents one class.\n",
        "Images are automatically labeled based on folder names and transformed according to the defined transformations.\n",
        "Extracting class names and their count for further processing/modeling.\n",
        "\"\"\"\n",
        "\n",
        "full_dataset = ImageFolder(root=data_path, transform=transform)\n",
        "class_names = full_dataset.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "\n",
        "# ======================== GRID SEARCH PARAMS\n",
        "\"\"\"\n",
        "Defining lists of potential hyperparameter values: learning rate, dropout rate, and batch size.\n",
        "Using itertools to create the full grid of parameter combinations (grid search),\n",
        "which will allow finding the optimal set for best model performance.\n",
        "\"\"\"\n",
        "learning_rates = [0.001, 0.0001]\n",
        "drop_rates = [0.3, 0.5]\n",
        "batch_sizes = [16, 32]\n",
        "\n",
        "# ======================== All hyperparameter combinations\n",
        "combinations = list(itertools.product(learning_rates, drop_rates, batch_sizes))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Main loop performing hyperparameter grid search.\n",
        "For each combination, the dataset is randomly split into train/val/test (70/15/15),\n",
        "new DataLoaders are created with the current batch size, and a new ConvNeXt-Atto model\n",
        "is initialized with the corresponding dropout rate.\n",
        "This allows evaluating how different settings affect model performance.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "for lr, drop_rate, batch_size in combinations:\n",
        "    print(f\"\\n========================> Training: lr={lr}, drop_rate={drop_rate}, batch_size={batch_size}\")\n",
        "\n",
        "    # ======================== DATA SPLIT EACH TIME (for randomness)\n",
        "    \"\"\"\n",
        "    Randomly splitting the dataset into training, validation, and test sets according to the predefined sizes.\n",
        "    Creating data loaders with the specified batch size (16) and parallel loading (2 workers).\n",
        "    Training data is shuffled each epoch, while validation and test data maintain a fixed order.\n",
        "    \"\"\"\n",
        "\n",
        "    train_size = int(0.7 * len(full_dataset))\n",
        "    val_size = int(0.15 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size - val_size\n",
        "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # ======================== MODEL\n",
        "    \"\"\"\n",
        "    Creating a ConvNeXt-Atto model from the timm library.\n",
        "    Model initialized with weights pretrained on ImageNet (transfer learning),\n",
        "    adjusted to the number of classes in the task.\n",
        "    Dropout and DropPath are set dynamically to test their effect during grid search.\n",
        "    Model is moved to the available device (GPU or CPU).\n",
        "    \"\"\"\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'convnext_atto',\n",
        "        pretrained=True,\n",
        "        num_classes=num_classes,\n",
        "        drop_rate=drop_rate,\n",
        "        drop_path_rate=0.2\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # ======================== FREEZING FEATURE EXTRACTOR\n",
        "    \"\"\"\n",
        "    Freezing all layers of the ConvNeXt model except the classification layer.\n",
        "    This allows only the last layer's weights (head) to be updated during training,\n",
        "    adapting the model to the specific task (e.g., tumor classification)\n",
        "    while keeping the features pretrained on ImageNet.\n",
        "    \"\"\"\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in model.get_classifier().parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "    # Only part of the earlier layers:\n",
        "\n",
        "    # for name, param in model.named_parameters():\n",
        "    # if \"blocks.29\" in name or \"head\" in name:  # e.g., last block + head\n",
        "    #     param.requires_grad = True\n",
        "\n",
        "\n",
        "    # To train the entire model:\n",
        "\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = True\n",
        "\n",
        "\n",
        "    # ======================== LOSS, OPTIMIZER\n",
        "    \"\"\"\n",
        "    Initializing Adam optimizer for the model's classification layer.\n",
        "    You can also use optim.SGD(..., momentum=0.9) or optim.AdamW(...)  # Adam with better regularization\n",
        "    Then, defining an `evaluate` function that performs model evaluation\n",
        "    on a given dataset (validation or test).\n",
        "    Returns average loss, accuracy, and weighted F1-score.\n",
        "    Turning off gradient tracking (`torch.no_grad()`) speeds up computation and reduces memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.get_classifier().parameters(), lr=lr)\n",
        "\n",
        "    def evaluate(loader):\n",
        "      model.eval()\n",
        "      total_loss = 0\n",
        "      all_preds, all_labels = [], []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for images, labels in loader:\n",
        "              images, labels = images.to(device), labels.to(device)\n",
        "              outputs = model(images)\n",
        "              loss = criterion(outputs, labels)\n",
        "              total_loss += loss.item()\n",
        "\n",
        "              _, preds = torch.max(outputs, 1)\n",
        "              all_preds.extend(preds.cpu().numpy())\n",
        "              all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "      acc = accuracy_score(all_labels, all_preds)\n",
        "      f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "      avg_loss = total_loss / len(loader)\n",
        "\n",
        "      return avg_loss, acc, f1\n",
        "\n",
        "\n",
        "\n",
        "    # ======================== TRAINING ========================\n",
        "    \"\"\"\n",
        "    Main training loop for the model over a specified number of epochs.\n",
        "\n",
        "    - Sets the model to training mode.\n",
        "    - Iterates over batches, performing forward and backward passes and updating weights.\n",
        "    - Collects and calculates metrics (loss, accuracy, F1) for training.\n",
        "    - After each epoch, calls the evaluation function on the validation set.\n",
        "    - Prints training and validation results.\n",
        "    - Creates a directory to save results (models, plots) with a name based on hyperparameters.\n",
        "    \"\"\"\n",
        "\n",
        "    history = {\n",
        "    'train_acc': [],\n",
        "    'val_acc': [],\n",
        "    'train_loss': [],\n",
        "    'val_loss': []\n",
        "                      }\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        train_acc = accuracy_score(all_labels, all_preds)\n",
        "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        val_loss, val_acc, val_f1 = evaluate(val_loader)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        print(f\"Train loss: {total_loss / len(train_loader):.4f}, acc: {train_acc:.4f}, f1: {train_f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}